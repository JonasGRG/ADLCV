{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increase channels to 3 torch.Size([32, 3, 224, 224])\n",
      "Swap batch with the channels torch.Size([3, 32, 224, 224])\n",
      "Merge batch with channels torch.Size([96, 224, 224])\n",
      "Split the batch to two dimensions torch.Size([16, 2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch_size, heigh, width = 32, 224,224\n",
    "a = torch.rand((batch_size, heigh, width))\n",
    "\n",
    "# Increase the channels to 3\n",
    "a1 = repeat(a, 'b h w -> b c h w', c=3)\n",
    "print('Increase channels to 3',a1.shape)\n",
    "\n",
    "# Swap batch with the channels\n",
    "a2 = rearrange(a1, 'b c h w -> c b h w')\n",
    "print('Swap batch with the channels', a2.shape)\n",
    "\n",
    "# Merge batch with channels\n",
    "a3 = rearrange(a1, 'b c h w -> (b c) h w')\n",
    "print('Merge batch with channels', a3.shape)\n",
    "\n",
    "\n",
    "# Split the batch to two dimensions\n",
    "a4 = rearrange(a1, '(d1 d2) c h w -> d1 d2 c h w', d1=16,d2=2)\n",
    "print('Split the batch to two dimensions', a4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(tensor=None):\n",
    "    if tensor is None:\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f'Embedding dimension ({embed_dim}) should be divisible by number of heads ({num_heads})'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.k_projection  = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.q_projection = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v_projeciton  = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.o_projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This function computes the multi-head self-attention of x.\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "\n",
    "        # Generate keys, queries, and values\n",
    "        keys    = self.k_projection(x) # B x seq_len x embed_dim\n",
    "        queries = self.q_projection(x) # B x seq_len x embed_dim\n",
    "        values  = self.v_projeciton(x) # B x seq_len x embed_dim\n",
    "\n",
    "        \"\"\"\n",
    "        Now you have to split the projected keys, queries, and values to multiple heads.\n",
    "        \"\"\"\n",
    "        # First split the embed_dim to num_heads x head_dim\n",
    "        keys = ...\n",
    "        # Secondly merge the batch_size with the num_heads\n",
    "        keys = ...\n",
    "        \n",
    "        # HINT repeat the same process for queries and values\n",
    "        queries = ...\n",
    "        queries = ...\n",
    "\n",
    "        values = ...\n",
    "        values = ...\n",
    "\n",
    "        # Compute attetion logits\n",
    "        attention_logits = ... # multiply queries and keys\n",
    "        attention_logits = attention_logits * self.scale\n",
    "        attention = ... # softmax on attention\n",
    "        out = ... # multiply attention with values\n",
    "\n",
    "        # Rearragne output\n",
    "        # from (batch_size x num_head) x seq_length x head_dim to batch_size x seq_length x embed_dim\n",
    "        out = rearrange(out, '(b h) s d -> b s (h d)', h=self.num_heads, d=self.head_dim)\n",
    "\n",
    "        assert attention.size() == (batch_size*self.num_heads, seq_length, seq_length)\n",
    "        assert out.size() == (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        return self.o_projection(out)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, fc_dim=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = Attention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        fc_hidden_dim = 4*embed_dim if fc_dim is None else fc_dim\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, fc_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_out = self.attention(x)\n",
    "        x = self.layernorm1(attention_out + x)\n",
    "        x = self.dropout(x)\n",
    "        fc_out = self.fc(x)\n",
    "        x = self.layernorm2(fc_out + x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_len=512):\n",
    "\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_seq_len, embed_dim)\n",
    "        position = torch.arange(0., max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        # get half of the embedding indices\n",
    "        div_term = torch.arange(0., embed_dim, 2)\n",
    "        # miltiply each position with -(math.log(10000.0) / embed_dim)\n",
    "        div_term = ...\n",
    "        # compute the exp of div_term\n",
    "        div_term = ...\n",
    "        pe[:, ...] = ... # HINT use torch.sin to assign to the even-positions the position * div_term\n",
    "        pe[:, ...] = ... # HINT use torch.cos to assign to the odd-positions the position * div_term\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        return x + self.pe[:, :seq_length]\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_len=512):\n",
    "\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.pe = nn.Embedding(embedding_dim=embed_dim, num_embeddings=max_seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        positions = self.pe(torch.arange(seq_length, device=to_device()))\n",
    "        positions = positions[None, :, :].expand(batch_size, seq_length, embed_dim)\n",
    "        return x + positions\n",
    "        \n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, max_seq_len,\n",
    "                 pos_enc='fixed', pool='cls', dropout=0.0, \n",
    "                 fc_dim=None, num_tokens=50_000, num_classes=2, \n",
    "                 \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pool in ['cls', 'mean', 'max']\n",
    "        assert pos_enc in ['fixed', 'learnable']\n",
    "        \n",
    "\n",
    "        self.pool, self.pos_enc, = pool, pos_enc\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=embed_dim, num_embeddings=num_tokens)\n",
    "\n",
    "        # Initialize cls token parameter\n",
    "        if self.pool == 'cls':\n",
    "            #self.cls_token = ... # Institate an nn.Parameter with size: 1,1,embed_dim\n",
    "            max_seq_len +=1\n",
    "        \n",
    "        if self.pos_enc == 'fixed':\n",
    "            self.positional_encoding = PositionalEncoding(embed_dim=embed_dim, max_seq_len=max_seq_len)\n",
    "        elif self.pos_enc == 'learnable':\n",
    "            self.positional_encoding = PositionalEmbedding(embed_dim=embed_dim, max_seq_len=max_seq_len)\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for i in range(num_layers):\n",
    "            transformer_blocks.append(\n",
    "                EncoderBlock(embed_dim=embed_dim, num_heads=num_heads, fc_dim=fc_dim, dropout=dropout))\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        tokens = self.token_embedding(x)\n",
    "        batch_size, seq_length, embed_dim = tokens.size()\n",
    "\n",
    "        # Include cls token in the input sequence\n",
    "        ####################### insert code here #######################\n",
    "        if self.pool == 'cls':\n",
    "            # HINT: repeat the cls token of the batch dimension\n",
    "            pass\n",
    "        ################################################################\n",
    "\n",
    "        x = self.positional_encoding(tokens)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        if self.pool =='max':\n",
    "            x = x.max(dim=1)[0]\n",
    "        elif self.pool =='mean':\n",
    "            x = x.mean(dim=1)\n",
    "            \n",
    "        # Get cls token\n",
    "        ####################### insert code here #######################\n",
    "        elif self.pool == 'cls':\n",
    "            # HINT: get the first output token of the transfomer.\n",
    "            pass\n",
    "        ################################################################\n",
    "\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlcv-ex-1-eWOusvJw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
